# Scalable test environment for load testing keycast.
#
# Uses HAProxy as load balancer with sticky sessions (Authorization header hashing).
# This simulates Cloud Run's session-affinity behavior for local testing.
# Each keycast instance gets 4 CPUs to match Cloud Run configuration.
#
# Usage (run from tools/loadtest/):
#   # Auto-scale based on available cores (4 cores per instance):
#   INSTANCES=$(($(nproc) / 4)) && docker-compose up -d --scale keycast=$INSTANCES
#
#   # Or specify manually:
#   docker-compose up -d --scale keycast=8

services:
  redis:
    image: redis:7-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 2s
      timeout: 3s
      retries: 5

  postgres:
    image: postgres:16
    environment:
      POSTGRES_PASSWORD: password
      POSTGRES_DB: keycast
    # No external port - accessed internally only
    # Tuned for high-concurrency OLTP (many small reads)
    command: >
      postgres
      -c max_connections=500
      -c shared_buffers=2GB
      -c effective_cache_size=8GB
      -c work_mem=64MB
      -c maintenance_work_mem=512MB
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8
      -c log_statement=none
      -c log_min_duration_statement=1000
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 2s
      timeout: 5s
      retries: 10
    volumes:
      - postgres_scale_data:/var/lib/postgresql/data
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '8'

  pgbouncer:
    # ongres/pgbouncer 1.24.0+ supports max_prepared_statements for sqlx
    image: ongres/pgbouncer:latest
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./pgbouncer-scale.ini:/etc/pgbouncer/pgbouncer.ini:ro
      - ./pgbouncer/userlist.txt:/etc/pgbouncer/userlist.txt:ro
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 2s
      timeout: 5s
      retries: 3
      start_period: 3s
    deploy:
      resources:
        limits:
          cpus: '2'

  # HAProxy provides sticky sessions via Authorization header hashing
  # This simulates Cloud Run's session-affinity behavior
  haproxy:
    image: haproxy:2.9-alpine
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "3000:80"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 5s
      timeout: 3s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '4'

  keycast:
    build: ../..
    depends_on:
      postgres:
        condition: service_healthy
      pgbouncer:
        condition: service_healthy
      redis:
        condition: service_healthy
    # Workaround for Tokio async DNS resolver issues with Docker
    # These entries are added to /etc/hosts for reliable resolution
    extra_hosts:
      - "pgbouncer:172.19.0.4"
      - "postgres:172.19.0.3"
      - "redis:172.19.0.2"
    environment:
      # Use pgbouncer for connection pooling (hardcoded - don't use env override)
      DATABASE_URL: postgres://postgres:password@172.19.0.4:5432/keycast
      # Direct connection for migrations (bypasses pgbouncer)
      DATABASE_DIRECT_URL: postgres://postgres:password@172.19.0.3:5432/keycast
      # Redis for cluster coordination
      REDIS_URL: redis://172.19.0.2:6379
      ALLOWED_ORIGINS: "*"
      SERVER_NSEC: ${SERVER_NSEC}
      MASTER_KEY_PATH: /app/master.key
      RUST_LOG: ${RUST_LOG:-warn,keycast=info}
      # Statement cache for prepared statements through PgBouncer
      SQLX_STATEMENT_CACHE: "100"
      # Disable emails for load testing
      DISABLE_EMAILS: "true"
      # Reduce log noise at scale
      RUST_BACKTRACE: 0
      # Skip migrations after first instance - prevents advisory lock contention
      SKIP_MIGRATIONS: ${SKIP_MIGRATIONS:-0}
      # Tokio worker threads - defaults to 4 for 8x4=32 core setup
      TOKIO_WORKER_THREADS: ${TOKIO_WORKER_THREADS:-4}
    volumes:
      - ./master.key:/app/master.key:ro
    # No port mapping - haproxy handles external access
    expose:
      - "3000"
    deploy:
      resources:
        limits:
          memory: 4G
          # 4 CPUs per instance - for 8 instances = 32 cores total
          cpus: '${KEYCAST_CPUS:-4}'
    # Auto-restart on failure (DNS may not be ready immediately)
    restart: on-failure:5
    # Container names are auto-generated: keycast-keycast-1, keycast-keycast-2, etc.

volumes:
  postgres_scale_data:
